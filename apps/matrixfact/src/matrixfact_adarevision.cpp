#include <vector>
#include <fstream>
#include <algorithm>
#include <iostream>
#include <cstdint>
#include <sstream>
#include <limits>
#include <random>
#include <set>
#include <thread>
#include <float.h>
#include <boost/thread/barrier.hpp>

#include <petuum/include/petuum_ps.hpp>
#include <petuum/include/system_gflags_declare.hpp>
#include <petuum/include/table_gflags_declare.hpp>
#include <petuum/include/init_table_config.hpp>
#include <petuum/include/init_table_group_config.hpp>
#include <petuum/ps_common/util/class_register.hpp>
#include <petuum/ps_common/oplog/dense_row_oplog.hpp>
#include <petuum/ps_common/index/row_id_set.hpp>
#include <petuum/include/abstract_server_table_logic.hpp>
#include <petuum/ps/server/adarevision_server_table_logic.hpp>
#include <gflags/gflags.h>
#include <glog/logging.h>

// Command-line flags
DECLARE_double(init_step_size);
DEFINE_double(lambda, 0.001, "L2 regularization strength.");
DEFINE_int32(K, 100, "Factorization rank");

DEFINE_string(datafile, "", "Input sparse matrix");
DEFINE_string(output_prefix, "", "Output results with this prefix. "
    "If the prefix is X, the output will be called X.L and X.R");
DEFINE_int32(num_iterations, 100, "Number of iterations");
DEFINE_int32(num_clocks_per_iter, 1,
             "clock num_clocks_per_iter in each SGD sweep.");
DEFINE_int32(num_clocks_per_eval, 1, "# of clocks between "
             "each objective evaluation");
DEFINE_int32(num_worker_threads, 1, "Number of worker threads per client");

DEFINE_bool(output_LR, false, "Save L and R matrices to disk or not.");

DEFINE_uint64(M_cache_size, 10000000, "Process cache size for the R table.");
DEFINE_uint64(M_client_send_oplog_upper_bound, 100, "M client upper bound");
DEFINE_int32(nnz_per_row, 1, "nnz per row");
DEFINE_int32(nnz_per_col, 1, "nnz per col");

// Data variables
size_t X_num_rows, X_num_cols; // Number of rows and cols. (L_table has N_ rows, R_table has M_ rows.)
std::vector<int> X_row; // Row index of each nonzero entry in the data matrix
std::vector<int> X_col; // Column index of each nonzero entry in the data matrix
std::vector<float> X_val; // Value of each nonzero entry in the data matrix
std::vector<int> X_partition_starts;
std::map<int32_t, size_t> nnz_per_row;
std::map<int32_t, size_t> nnz_per_col;

int kLossTableColIdxClock = 0;
int kLossTableColIdxComputeTime = 1;
int kLossTableColIdxL2Loss = 2;
int kLossTableColIdxL2RegLoss = 3;
int kLossTableColIdxComputeEvalTime = 4;
int kLossTableColIdxIter = 5;

void ReadBinaryMatrix(const std::string &filename, int32_t partition_id) {
  std::string bin_file = filename + "." + std::to_string(partition_id);

  FILE *bin_input = fopen(bin_file.c_str(), "rb");
  CHECK(bin_input != 0) << "failed to read " << bin_file;

  size_t num_nnz_this_partition = 0,
        num_rows_this_partition = 0,
        num_cols_this_partition = 0;
  size_t read_size = fread(&num_nnz_this_partition, sizeof(size_t), 1, bin_input);
  CHECK_EQ(read_size, 1);
  read_size = fread(&num_rows_this_partition, sizeof(size_t), 1, bin_input);
  CHECK_EQ(read_size, 1);
  read_size = fread(&num_cols_this_partition, sizeof(size_t), 1, bin_input);
  CHECK_EQ(read_size, 1);

  X_row.resize(num_nnz_this_partition);
  X_col.resize(num_nnz_this_partition);
  X_val.resize(num_nnz_this_partition);

  read_size = fread(X_row.data(), sizeof(int), num_nnz_this_partition, bin_input);
  CHECK_EQ(read_size, num_nnz_this_partition);
  read_size = fread(X_col.data(), sizeof(int), num_nnz_this_partition, bin_input);
  CHECK_EQ(read_size, num_nnz_this_partition);
  read_size = fread(X_val.data(), sizeof(float), num_nnz_this_partition, bin_input);
  CHECK_EQ(read_size, num_nnz_this_partition);

  X_num_rows = num_rows_this_partition;
  X_num_cols = num_cols_this_partition;
  LOG(INFO) << "partition = " << partition_id
            << " #row = " << X_num_rows
            << " #cols = " << X_num_cols;
}

void PartitionWorkLoad(int32_t num_local_workers) {
  size_t num_nnz = X_val.size();
  size_t num_nnz_per_partition = num_nnz / num_local_workers;

  X_partition_starts.resize(num_local_workers);
  int64_t partition_start = 0;
  for (int32_t i = 0; i < num_local_workers; ++i) {
    X_partition_starts[i] = partition_start;

    int64_t partition_end = (i == num_local_workers - 1) ?
                            num_nnz - 1 :
                            partition_start + num_nnz_per_partition;

    int end_row_id = X_row[partition_end];
    CHECK(partition_end < num_nnz) << "i = " << i;

    if (i != num_local_workers - 1) {
      while (partition_end < num_nnz
             && end_row_id == X_row[partition_end]) {
        ++partition_end;
      }

      CHECK(partition_end < num_nnz) << "There's empty bin " << i;
      partition_end--;
      partition_start = partition_end + 1;
    }
  }
}

void CountNNZPerRowCol() {
  for (auto &row : X_row) {
    nnz_per_row[row]++;
  }
  //LOG(INFO) << "row 1436 nnz = " << nnz_per_row[1436];

  for (auto &col : X_col) {
    nnz_per_col[col]++;
  }
}

// Returns the number of workers threads across all clients
int get_total_num_workers() {
  return FLAGS_num_clients * FLAGS_num_worker_threads;
}
// Returns the global thread ID of this worker
int get_global_worker_id(int thread_id) {
  return (FLAGS_client_id * FLAGS_num_worker_threads) + thread_id;
}

void RowToString(
    const std::vector<float> &row,
    std::string *str) {
  *str = "";
  for (int i = 0; i < row.size(); ++i) {
    *str += std::to_string(row[i]);
    *str += " ";
  }
}

void TakeSnapShot(const std::string &path,
                  const std::vector<std::vector<float> > &L_table) {
  //LOG(INFO) << __func__ << " size = " << L_table.size();
  {
    std::ofstream text_file;
    text_file.open(path + ".L");
    CHECK(text_file.good());
    std::string str;
    for (int i = 0; i < L_table.size(); ++i) {
      const auto &row_vec = L_table[i];
      RowToString(row_vec, &str);
      text_file << i
                << " " << str << "\n";
    }
    text_file.close();
  }
}

void InitLTable(size_t row_capacity,
                int32_t local_worker_id,
                std::vector<std::vector<float> > *L_table,
                std::vector<std::vector<float> > *L_hist_gradients,
                size_t *row_id_offset) {
  int row_id_start = X_row[X_partition_starts[local_worker_id]];
  int row_id_end = (local_worker_id == X_partition_starts.size() - 1) ?
                   X_row.back() :
                   X_row[X_partition_starts[local_worker_id + 1] - 1];

  size_t L_table_size = row_id_end - row_id_start + 1;
  L_table->resize(L_table_size, std::vector<float>(row_capacity, 0.0));
  L_hist_gradients->resize(L_table_size, std::vector<float>(row_capacity, 1));

  *row_id_offset = row_id_start;
}

// Performs stochastic gradient descent on X_row[a], X_col[a], X_val[a].
void SgdElement(
    int64_t a, int iter, float step_size, int global_worker_id,
    std::vector<std::vector<float> > &L_table,
    std::vector<std::vector<float> > &L_hist_gradients,
    size_t L_row_id_offset,
    petuum::Table<float>& R_table,
    petuum::Table<float>& loss_table,
    std::vector<float>* Rj_cache) {
  // Let i = X_row[a], j = X_col[a], and X(i,j) = X_val[a]
  const int i = X_row[a];
  const int j = X_col[a];
  const float Xij = X_val[a];
  //LOG(INFO) << __func__ << " i = " << i << " j = " << j;
  // Read R(:,j) from Petuum PS
  {
    //petuum::RowAccessor Rj_acc;
    //const auto& Rj_row = R_table.Get<petuum::DenseRow<float> >(j, &Rj_acc);
    //Rj_row.CopyToVector(Rj_cache);
    // Release the accessor.
  }
  auto& Rj = *Rj_cache;
  auto& Li = L_table[i - L_row_id_offset];

  // Compute L(i,:) * R(:,j)
  float LiRj = 0.0;
  for (int k = 0; k < FLAGS_K; ++k) {
    LiRj += Li[k] * Rj[k];
  }

  // Now update L(i,:) and R(:,j) based on the loss function at X(i,j).
  // The loss function at X(i,j) is ( X(i,j) - L(i,:)*R(:,j) )^2.
  //
  // The gradient w.r.t. L(i,k) is -2*X(i,j)R(k,j) + 2*L(i,:)*R(:,j)*R(k,j).
  // The gradient w.r.t. R(k,j) is -2*X(i,j)L(i,k) + 2*L(i,:)*R(:,j)*L(i,k).
  petuum::DenseUpdateBatch<float> Rj_update(0, FLAGS_K);
  float grad_coeff = -(Xij - LiRj);
  float regularization_coeff = FLAGS_lambda;
  auto &L_hist_gradients_row = L_hist_gradients[i - L_row_id_offset];
  for (int k = 0; k < FLAGS_K; ++k) {
    // Compute update for L(i,k)
    float L_gradient = 2 * (grad_coeff * Rj[k] + regularization_coeff / float(FLAGS_nnz_per_row) * Li[k]);
    float R_gradient = 2 * (grad_coeff * Li[k] + regularization_coeff / float(FLAGS_nnz_per_col) * Rj[k]);

    L_hist_gradients_row[k] += L_gradient * L_gradient;

    L_hist_gradients_row[k] = L_hist_gradients_row[k];

    CHECK(L_gradient == L_gradient) << "client = " << FLAGS_client_id
    				    << " grad_coeff = " << grad_coeff
				    << " L_gradient = " << L_gradient
				    << " Lhg = " << L_hist_gradients_row[k]
				    << " a = " << a
				    << " k = " << k
    				    << " R_" << j
    				    << "_[" << k << "] = " << Rj[k]
    				    << " L_" << i
    				    << "_[" << k << "] = " << Li[k]
				    << " LiRj = " << LiRj
				    << " Xij = " << Xij;

    Li[k] -= step_size / sqrt(L_hist_gradients_row[k]) * L_gradient;

    Li[k] = Li[k];

    CHECK(Li[k] == Li[k])  << "client = " << FLAGS_client_id
   			   << " grad_coeff = " << grad_coeff
    			   << " L_gradient = " << L_gradient
    			   << " a = " << a
   			   << " k = " << k
    			   << " R_" << j
    			   << "_[" << k << "] = " << Rj[k];

    // Compute update for R(k,j)
    CHECK(R_gradient == R_gradient) << "client = " << FLAGS_client_id
				    << " gradient = " << R_gradient
				    << " a = " << a
				    << " k = " << k
				    << " hist_gradients = " << L_hist_gradients_row[k]
				    << " LiRj = " << LiRj
				    << " Xij = " << Xij;

    Rj_update[k] += R_gradient;
  }
  R_table.DenseBatchInc(j, Rj_update);
}

void InitMF(
    std::vector<std::vector<float> >& L_table,
    petuum::Table<float>& R_table,
    int col_begin, int col_end) {
  // Create a uniform RNG in the range (-1,1)
  //std::random_device rd;
  std::mt19937 gen(12345);
  std::normal_distribution<float> dist(0, 0.1);

  for (auto &L_row : L_table) {
    for (int k = 0; k < FLAGS_K; ++k) {
      L_row[k] = dist(gen);
    }
  }
}

std::string GetExperimentInfo() {
  std::stringstream ss;
  ss << "Rank(K) = " << FLAGS_K << std::endl
    << "Matrix dimensions: " << X_num_rows << " by " << X_num_cols << std::endl
    << "# non-missing entries: " << X_row.size() << std::endl
    << "num_iterations = " << FLAGS_num_iterations << std::endl
    << "num_clients = " << FLAGS_num_clients << std::endl
    << "num_worker_threads = " << FLAGS_num_worker_threads << std::endl
    << "num_comm_channels_per_client = "
    << FLAGS_num_comm_channels_per_client << std::endl
    << "num_clocks_per_iter = " << FLAGS_num_clocks_per_iter << std::endl
    << "num_clocks_per_eval = " << FLAGS_num_clocks_per_eval << std::endl
    << "M_cache_size = " << FLAGS_M_cache_size << std::endl
    << "staleness = " << FLAGS_staleness << std::endl
    << "ssp_mode = " << FLAGS_consistency_model << std::endl
    << "init_step_size = " << FLAGS_init_step_size << std::endl
    << "lambda = " << FLAGS_lambda << std::endl
    << "data file = " << FLAGS_datafile << std::endl;
  return ss.str();
}

void OutputToDisk(petuum::Table<float> &R_table,
                  petuum::Table<float>& loss_table,
                  int num_obj_evals, bool final) {
  static int record_counter = 0;
  // loss_table
  std::string loss_file;
  if (final) {
    loss_file = FLAGS_output_prefix + ".loss";
  } else {
    loss_file = FLAGS_output_prefix + ".loss" + std::to_string(record_counter);
  }
  LOG(INFO) << "Writing to loss file: " << loss_file;
  std::ofstream loss_stream(loss_file.c_str());
  loss_stream << GetExperimentInfo();
  loss_stream << "Iter Clock Compute-Time Compute-Eval-Time L2_loss "
    << "L2_regularized_loss" << std::endl;
  for (int obj_eval_counter = 0; obj_eval_counter < num_obj_evals;
      ++obj_eval_counter) {
    //petuum::RowAccessor loss_acc;
    //loss_table.Get(obj_eval_counter, &loss_acc);
    //const auto& loss_row = loss_acc.Get<petuum::DenseRow<float> >();
    //loss_stream << loss_row[kLossTableColIdxIter] << " "
    //  << loss_row[kLossTableColIdxClock] << " "
    //  << loss_row[kLossTableColIdxComputeTime] << " "
    //  << loss_row[kLossTableColIdxComputeEvalTime] << " "
    //  << loss_row[kLossTableColIdxL2Loss] << " "
    //  << loss_row[kLossTableColIdxL2RegLoss] << std::endl;
  }
  loss_stream.close();
  ++record_counter;
}

void RecordLoss(
    int obj_eval_counter, int iter, int clock,
    std::vector<std::vector<float> > &L_table,
    size_t L_row_id_offset,
    petuum::Table<float>& R_table,
    petuum::Table<float>& loss_table,
    int col_begin, int col_end,
    int global_worker_id, int total_num_workers,
    int element_begin, int element_end,
    std::vector<float>* Rj_cache) {
  float squared_loss = 0.;
  // for (int a = global_worker_id; a < X_row.size(); a += total_num_workers) {
  for (int a = element_begin; a < element_end; ++a) {
    // Let i = X_row[a], j = X_col[a], and X(i,j) = X_val[a]
    const int i = X_row[a];
    //const int j = X_col[a];
    const float Xij = X_val[a];
    // Read L(i,:) and R(:,j) from Petuum PS
    {
      //petuum::RowAccessor Rj_acc;
      //const auto& Rj_row = R_table.Get<petuum::DenseRow<float> >(j, &Rj_acc);
      //Rj_row.CopyToVector(Rj_cache);
    }
    auto& Li = L_table[i - L_row_id_offset];
    auto& Rj = *Rj_cache;

    // Compute L(i,:) * R(:,j)
    float LiRj = 0.0;
    for (int k = 0; k < FLAGS_K; ++k) {
      LiRj += Li[k] * Rj[k];
    }

    // Update the L2 (non-regularized version) loss function
    squared_loss += pow(Xij - LiRj, 2);
  }
  loss_table.Inc(obj_eval_counter, kLossTableColIdxL2Loss, squared_loss);

  if (global_worker_id == 0) {
    loss_table.Inc(obj_eval_counter, kLossTableColIdxClock, clock);
    loss_table.Inc(obj_eval_counter, kLossTableColIdxIter, iter);
  }

  //LOG(INFO) << "squared loss = " << squared_loss;

  // Compute loss.
  float L2_reg_loss = 0.;
  for (auto &Li : L_table) {
    for (int k = 0; k < FLAGS_K; ++k) {
      L2_reg_loss += Li[k] * Li[k];
    }
  }

  //LOG(INFO) << "reg_loss = " << L2_reg_loss;

  for (int ii = col_begin; ii < col_end; ++ii) {
    {
      //petuum::RowAccessor Rj_acc;
      //const auto& Rj_row
      //    = R_table.Get<petuum::DenseRow<float> >(ii, &Rj_acc);
      //Rj_row.CopyToVector(Rj_cache);
    }
    auto &Rj = *Rj_cache;

    for (int k = 0; k < FLAGS_K; ++k) {
      L2_reg_loss += Rj[k] * Rj[k];
    }
  }

  L2_reg_loss *= FLAGS_lambda;
  loss_table.Inc(obj_eval_counter, kLossTableColIdxL2RegLoss,
                 L2_reg_loss + squared_loss);
}

// Main Matrix Factorization routine, called by pthread_create
void SolveMF(int32_t thread_id, boost::barrier* process_barrier) {
  // Register this thread with Petuum PS
  petuum::PSTableGroup::RegisterThread();
  // Get tables
  petuum::Table<float> R_table = petuum::PSTableGroup::GetTableOrDie<float>(1);
  petuum::Table<float> loss_table =
      petuum::PSTableGroup::GetTableOrDie<float>(2);

  std::vector<std::vector<float> > L_table;
  std::vector<std::vector<float> > L_hist_gradients;
  size_t row_id_offset = 0;
  InitLTable(FLAGS_K, thread_id, &L_table,
             &L_hist_gradients, &row_id_offset);

  const int total_num_workers = get_total_num_workers();
  // global_worker_id lies in the range [0,total_num_workers)
  const int global_worker_id = get_global_worker_id(thread_id);

  int num_cols_per_thread = X_num_cols / total_num_workers;
  int col_begin = global_worker_id * num_cols_per_thread;
  int col_end = (global_worker_id == total_num_workers - 1) ?
                X_num_cols : col_begin + num_cols_per_thread;

  // Cache for DenseRow bulk-read
  std::vector<float> Rj_cache(FLAGS_K);

  STATS_APP_INIT_BEGIN();
  // Initialize MF solver
  InitMF(L_table, R_table, col_begin, col_end);

  // Run additional iterations to let stale values finish propagating
  petuum::PSTableGroup::GlobalBarrier();
  STATS_APP_INIT_END();

  // Run MF solver
  int clock = 0;
  petuum::HighResolutionTimer total_timer;
  double total_eval_time = 0.;
  int obj_eval_counter = 0;  // ith objective evaluation

  int64_t element_begin = X_partition_starts[thread_id];
  int64_t element_end = (thread_id == FLAGS_num_worker_threads - 1) ?
                    X_row.size() : X_partition_starts[thread_id + 1];

  // round down in order to have same # of clocks per iteration.
  int64_t work_per_clock = (element_end - element_begin) / FLAGS_num_clocks_per_iter;
  CHECK(work_per_clock > 0) << "work_per_clock is less than 1"
                            << " reduce num_clocks_per_iter "
                            << " X_row.size() = " << X_row.size()
                            << " total_num_workers = " << total_num_workers;

  // Bootstrap.
  if (thread_id == 0) {
    LOG(INFO) << "Bootstrap starting";
    petuum::HighResolutionTimer bootstrap_timer;
    STATS_APP_BOOTSTRAP_BEGIN();
    std::set<int32_t> R_rows;
    // Last element on this machine.
    int64_t process_element_end = X_row.size();

    for (int64_t a = 0; a < process_element_end; ++a) {
      R_rows.insert(X_col[a]);
    }

    //for (const auto& e : R_rows) {
      //R_table.GetAsyncForced(e);
    //}

    //R_table.WaitPendingAsyncGet();
    STATS_APP_BOOTSTRAP_END();
    LOG(INFO) << "Bootstrap finished in " << bootstrap_timer.elapsed()
              << " seconds.";
  }
  process_barrier->wait();

  petuum::PSTableGroup::GlobalBarrier();

  petuum::PSTableGroup::TurnOnEarlyComm();

  for (int iter = 0; iter < FLAGS_num_iterations; ++iter) {
    if (global_worker_id == 0) {
      LOG(INFO) << "Iteration " << iter+1 << "/"
                << FLAGS_num_iterations << "... ";
    }

    size_t element_counter = 0;
    float step_size = FLAGS_init_step_size;

    // Overall computation (no eval / communication time)
    STATS_APP_ACCUM_COMP_BEGIN();
    for (int64_t a = element_begin; a < element_end; ++a) {
      SgdElement(a, iter, step_size, global_worker_id, L_table,
                 L_hist_gradients,
                 row_id_offset, R_table, loss_table, &Rj_cache);
      ++element_counter;

      if ((element_counter % work_per_clock == 0
           && clock < (iter + 1)*FLAGS_num_clocks_per_iter - 1)
          || (element_counter == element_end - element_begin)) {
        petuum::PSTableGroup::Clock();
        ++clock;
        STATS_APP_ACCUM_COMP_END();

        // Do a fake get to avoid initial block time.
        //const int i = X_col[a];
        //petuum::RowAccessor Ri_acc;
        //R_table.Get(i, &Ri_acc);

        // Evaluate (if needed) before clocking.
        if (clock % FLAGS_num_clocks_per_eval == 0) {
          STATS_APP_ACCUM_OBJ_COMP_BEGIN();
          petuum::HighResolutionTimer eval_timer;
          // Record objective at each clock
          RecordLoss(obj_eval_counter, iter+1, clock, L_table,
                     row_id_offset, R_table,
                     loss_table, col_begin, col_end,
                     global_worker_id, total_num_workers,
                     element_begin, element_end, &Rj_cache);
          float eval_cost = eval_timer.elapsed();
          total_eval_time += eval_cost;
          STATS_APP_ACCUM_OBJ_COMP_END();

          if (global_worker_id == 0 && obj_eval_counter > 0) {
            // Read the obj value from last recording (which is fresh as loss
            // table has 0 staleness).
            {
              //petuum::RowAccessor loss_acc;
              //const petuum::DenseRow<float>& loss_row =
              //  loss_table.Get<petuum::DenseRow<float> >(obj_eval_counter - 1, &loss_acc);
              //LOG(INFO) << "End of clock " << clock
              //<< " L2_loss = " << loss_row[kLossTableColIdxL2Loss]
              //<< " L2_reg_loss = " << loss_row[kLossTableColIdxL2RegLoss];
              // Release loss_acc.
            }

            // Output to disk in case of failure.
            OutputToDisk(R_table, loss_table, obj_eval_counter,
                false);
            float total_time = total_timer.elapsed();
            float compute_time = total_time - total_eval_time;
            LOG(INFO) << " Evaluation cost = " << eval_cost
              << " || Total compute time = " << compute_time
              << " || Total time = " << total_time;
            // Record approximate end-of-this-clock time (using head thread's
            // finish time).
            loss_table.Inc(obj_eval_counter, kLossTableColIdxComputeTime,
                compute_time);
            loss_table.Inc(obj_eval_counter, kLossTableColIdxComputeEvalTime,
                total_time);
          }
          ++obj_eval_counter;
        }

        // Overall computation (no eval / communication time)
        STATS_APP_ACCUM_COMP_BEGIN();
      }
    }
    CHECK_EQ((iter+1)*FLAGS_num_clocks_per_iter, clock);
    CHECK_EQ(clock / FLAGS_num_clocks_per_eval, obj_eval_counter);
    LOG_IF(INFO, global_worker_id == 0) << "Iter " << iter+1
      << " finished. Time: " << total_timer.elapsed();
  }

  petuum::PSTableGroup::TurnOffEarlyComm();

  // Finish propagation
  petuum::PSTableGroup::GlobalBarrier();
  //TakeSnapShot("L.test.out", L_table);

  // Output loss function
  if (global_worker_id == 0) {
    std::stringstream ss;
    ss << "Iter Clock Compute-Time Compute-Eval-Time L2_loss L2_reg_loss"
      << std::endl;
    for (int c = 0; c < obj_eval_counter; ++c) {
      //petuum::RowAccessor loss_acc;
      //const auto& loss_row = loss_table.Get<petuum::DenseRow<float> >(c, &loss_acc);
      //ss << loss_row[kLossTableColIdxIter] << " "
      //  << loss_row[kLossTableColIdxClock] << " "
      //  << loss_row[kLossTableColIdxComputeTime] << " "
      //  << loss_row[kLossTableColIdxComputeEvalTime] << " "
      //  << loss_row[kLossTableColIdxL2Loss] << " "
      //  << loss_row[kLossTableColIdxL2RegLoss] << std::endl;
    }
    LOG(INFO) << "Summary Stats = \n" << ss.str();
  }

  // Output results to disk
  if (global_worker_id == 0) {
    LOG(INFO) << "Outputting results to prefix " << FLAGS_output_prefix
      << " ... ";
    OutputToDisk(R_table, loss_table, obj_eval_counter, true);
    LOG(INFO) << "done";
  }
  // Deregister this thread with Petuum PS
  petuum::PSTableGroup::DeregisterThread();
}

// Main function
int main(int argc, char *argv[]) {
  google::ParseCommandLineFlags(&argc, &argv, true);
  google::InitGoogleLogging(argv[0]);
  petuum::HighResolutionTimer total_timer;

  // Configure Petuum PS
  petuum::TableGroupConfig table_group_config;
  petuum::InitTableGroupConfig(&table_group_config, 2);

  // Configure PS row types
  // Register dense float rows as ID 0
  petuum::ClassRegistry<petuum::AbstractServerTableLogic>::GetRegistry().AddCreator(
      1, petuum::CreateObj<petuum::AbstractServerTableLogic,
      petuum::AdaRevisionServerTableLogic>);

  petuum::PSTableGroup::RegisterRow<petuum::DenseRow<float> >(0);
  petuum::PSTableGroup::RegisterRow<petuum::DenseRow<int64_t> >(1);

  petuum::PSTableGroup::RegisterRowOpLog<petuum::DenseRowOpLog<float> >(0);

  // Initializing thread does not need table access
  petuum::PSTableGroup::Init(table_group_config, false);

  LOG(INFO) << "TableGroupInit is done";

  // Load Data
  if (FLAGS_client_id == 0) {
    LOG(INFO) << std::endl << "Loading data... ";
  }

  LOG(INFO) << "Read data " << FLAGS_datafile;

  ReadBinaryMatrix(FLAGS_datafile, FLAGS_client_id);
  PartitionWorkLoad(FLAGS_num_worker_threads);
  CountNNZPerRowCol();

  if (FLAGS_client_id == 0) {
    LOG(INFO) << std::endl << GetExperimentInfo();
  }

  // Configure PS tables
  petuum::ClientTableConfig table_config;
  petuum::InitTableConfig(&table_config);

  table_config.table_info.server_push_row_upper_bound
      = FLAGS_server_push_row_upper_bound;

  // R_table (M by K)
  table_config.table_info.row_capacity = FLAGS_K;
  table_config.cache_capacity = FLAGS_M_cache_size;
  petuum::RowIDSetHeader *row_id_set = petuum::RowIDSetFactory::CreateRowIDSet(
      petuum::Dense, FLAGS_M_cache_size);
  table_config.row_id_set = row_id_set;
  table_config.client_send_oplog_upper_bound
      = FLAGS_M_client_send_oplog_upper_bound;
  petuum::PSTableGroup::CreateTable(1, table_config);
  petuum::RowIDSetFactory::FreeRowIDSet(row_id_set);

  table_config.table_info.oplog_dense_serialized = true;
  table_config.index_type = petuum::Dense;
  table_config.table_info.staleness = FLAGS_staleness;
  petuum::RowIDSetHeader *row_id_set2 = petuum::RowIDSetFactory::CreateRowIDSet(
      petuum::Dense, 10);
  table_config.table_info.row_capacity = 6;
  table_config.table_info.row_oplog_type = 0;
  table_config.table_info.server_table_logic = -1;
  table_config.table_info.version_maintain = false;
  table_config.cache_capacity = 100;
  table_config.client_send_oplog_upper_bound = 1;
  petuum::PSTableGroup::CreateTable(2, table_config);
  petuum::RowIDSetFactory::FreeRowIDSet(row_id_set2);

  LOG(INFO) << "Created all tables";
  // Finished creating tables
  petuum::PSTableGroup::CreateTableDone();

  // Run Petuum PS-based MF solver
  std::vector<std::thread> threads(FLAGS_num_worker_threads);
  boost::barrier process_barrier(FLAGS_num_worker_threads);
  for (int t = 0; t < FLAGS_num_worker_threads; ++t) {
    threads[t] = std::thread(SolveMF, t, &process_barrier);
  }
  for (auto& thr : threads) {
    thr.join();
  }

  // Cleanup and output runtime
  petuum::PSTableGroup::ShutDown();
  if (FLAGS_client_id == 0) {
    LOG(INFO) << "total runtime = " << total_timer.elapsed() << "s";
  }

  LOG(INFO) << "exiting " << FLAGS_client_id;
  return 0;
}
